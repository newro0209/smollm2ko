from __future__ import annotations

import os
from collections import Counter
from functools import reduce
from multiprocessing import Pool
from typing import Any

import torch
from datasets import load_dataset
from tqdm.auto import tqdm
from transformers import AutoTokenizer, BatchEncoding, PreTrainedTokenizerBase

MODEL_NAME = "HuggingFaceTB/SmolLM2-135M-Instruct"
SFT_DATASET = "HuggingFaceTB/smol-smoltalk"
SEED = 42
TOP_N = 30
SFT_SAMPLE_LIMIT = 10_000


def extract_text_from_sample(sample: dict[str, Any], columns: set[str]) -> str:
    """ìƒ˜í”Œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if "text" in columns:
        return str(sample.get("text") or "")

    if "messages" in columns:
        messages = sample.get("messages") or []
        return "\n".join(
            f"{msg.get('role', 'user')}: {msg.get('content', '')}" for msg in messages
        )

    return " ".join(str(value) for value in sample.values())


def process_batch(
    batch: dict[str, Any], tokenizer: PreTrainedTokenizerBase
) -> BatchEncoding:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í† í°í™”í•©ë‹ˆë‹¤."""
    columns = set(batch.keys())
    batch_size = len(next(iter(batch.values())))

    texts = [
        extract_text_from_sample({col: batch[col][i] for col in columns}, columns)
        for i in range(batch_size)
    ]

    return tokenizer(
        texts,
        add_special_tokens=False,
        truncation=False,
        max_length=None,
        return_attention_mask=False,
    )


def count_tokens_chunk(token_ids_list: list[list[int]]) -> Counter[int]:
    """í† í° ID ë¦¬ìŠ¤íŠ¸ì˜ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ Counterë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    counter: Counter[int] = Counter()
    for ids in token_ids_list:
        counter.update(ids)
    return counter


def parallel_count_tokens(
    token_ids_dataset: list[list[int]],
    num_proc: int,
) -> Counter[int]:
    """ë³‘ë ¬ ì²˜ë¦¬ë¡œ í† í°ì„ ì§‘ê³„í•©ë‹ˆë‹¤."""
    chunk_size = max(1, len(token_ids_dataset) // num_proc)
    chunks = [
        token_ids_dataset[i : i + chunk_size]
        for i in range(0, len(token_ids_dataset), chunk_size)
    ]

    with Pool(processes=num_proc) as pool:
        counters = list(
            tqdm(
                pool.imap(count_tokens_chunk, chunks),
                total=len(chunks),
                desc="ğŸ“Š í† í° ì§‘ê³„ (ë³‘ë ¬)",
            )
        )

    return reduce(lambda a, b: a + b, counters, Counter())


def print_report(
    tokenizer: PreTrainedTokenizerBase,
    token_counter: Counter[int],
    top_n: int,
) -> None:
    """í† í° ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    vocab_size = len(tokenizer)
    unused_tokens = [
        token_id for token_id in range(vocab_size) if token_id not in token_counter
    ]
    low_freq_tokens = token_counter.most_common()[-top_n:][::-1]

    # ë¯¸ì‚¬ìš© í† í°
    print("\n[SFT] ë¯¸ì‚¬ìš© í† í° ëª©ë¡")
    print(f"COUNT={len(unused_tokens)}")
    print("TOKEN_IDS=" + ", ".join(map(str, unused_tokens[:top_n])))

    # ì €ë¹ˆë„ í† í°
    print("\n[SFT] ì €ë¹ˆë„ í† í° ìƒìœ„ ëª©ë¡")
    for rank, (token_id, count) in enumerate(low_freq_tokens, start=1):
        token_str = tokenizer.decode([token_id], clean_up_tokenization_spaces=False)
        print(f"{rank:02d}. TOKEN_ID={token_id} | STR='{token_str}' | OCC={count}")

    # ì¬ë°°ì¹˜ í›„ë³´
    print("\n[SFT] ë³´ìº¡ ì¬ë°°ì¹˜ í›„ë³´")
    print("UNUSED_TOP=" + ", ".join(map(str, unused_tokens[:top_n])))
    print("LOW_FREQ_TOP=" + ", ".join(str(tid) for tid, _ in low_freq_tokens))


def main() -> None:
    """CLI ì‹¤í–‰ ì§„ì…ì ."""
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ“š í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(MODEL_NAME)

    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {SFT_DATASET}")
    dataset = load_dataset(SFT_DATASET, split="train")

    if SFT_SAMPLE_LIMIT and len(dataset) > SFT_SAMPLE_LIMIT:
        dataset = dataset.select(range(SFT_SAMPLE_LIMIT))
        print(f"âœ‚ï¸  ìƒ˜í”Œ ì œí•œ ì ìš©: {SFT_SAMPLE_LIMIT:,}ê±´")

    print(f"ğŸ“ ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset):,}ê±´")

    # ë³‘ë ¬ í† í°í™”
    num_proc = max(1, (os.cpu_count() or 1) // 2)
    print(f"âš™ï¸  ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ({num_proc} í”„ë¡œì„¸ìŠ¤)...")

    processed_dataset = dataset.map(
        lambda batch: process_batch(batch, tokenizer),
        batched=True,
        num_proc=num_proc,
        remove_columns=dataset.column_names,
        desc="ğŸ”„ í† í°í™” ì§„í–‰",
    )

    # í† í° ì¹´ìš´íŒ… (ë³‘ë ¬)
    token_counter = parallel_count_tokens(processed_dataset["input_ids"], num_proc)

    # ê²°ê³¼ ì¶œë ¥
    print_report(tokenizer, token_counter, TOP_N)


if __name__ == "__main__":
    main()
